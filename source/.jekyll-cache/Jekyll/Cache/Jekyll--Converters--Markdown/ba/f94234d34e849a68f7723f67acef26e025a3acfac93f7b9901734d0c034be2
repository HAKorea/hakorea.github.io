I"†(<p>The <code>tensorflow</code> image processing platform allows you to detect and recognize objects in a camera image using <a href="https://www.tensorflow.org/">TensorFlow</a>. The state of the entity is the number of objects detected, and recognized objects are listed in the <code>summary</code> attribute along with quantity. The <code>matches</code> attribute provides the confidence <code>score</code> for recognition and the bounding <code>box</code> of the object for each detection category.</p>
<div class='note warning'>
<p>The following packages must be installed on Raspbian before following the setup for the integration to work:
<code>sudo apt-get install libatlas-base-dev libopenjp2-7 libtiff5</code></p>
</div>
<h2>Setup</h2>
<p>You need to install the <code>tensorflow</code> Python packages with: <code>$ pip3 install tensorflow==1.13.2</code>. The wheel is not available for all platforms. See <a href="https://www.tensorflow.org/install/">the official install guide</a> for other options. Hass.io is not yet supported but an addon is under development.</p>
<p>This integration requires files to be downloaded, compiled on your computer, and added to the Home Assistant configuration directory. These steps can be performed using the sample script at <a href="https://gist.github.com/hunterjm/6f9332f92b60c3d5e448ad936d7353c3">this gist</a>. Alternatively, if you wish to perform the process manually, the process is as follows:</p>
<ul>
<li>Clone <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">tensorflow/models</a></li>
<li>Compile protobuf models located in <code>research/object_detection/protos</code> with <code>protoc</code></li>
<li>Create the following directory structure inside your config directory:</li>
</ul>
<pre><code class="language-bash">  |- {config_dir}
    | - tensorflow/
      |- object_detection/
        |- __init__.py
</code></pre>
<ul>
<li>
<p>Copy required object_detection dependencies to the <code>object_detection</code> folder inside of the <code>tensorflow</code> folder:</p>
<ul>
<li><code>research/object_detection/data</code></li>
<li><code>research/object_detection/utils</code></li>
<li><code>research/object_detection/protos</code></li>
</ul>
</li>
</ul>
<h2>Model Selection</h2>
<p>Lastly, it is time to pick a model. It is recommended to start with one of the COCO models available in the <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">Model Detection Zoo</a>.</p>
<p>The trade-off between the different models is accuracy vs speed.  Users with a decent CPU should start with the <code>faster_rcnn_inception_v2_coco</code> model. If you are running on an ARM device like a Raspberry Pi, start with the <code>ssd_mobilenet_v2_coco</code> model.</p>
<p>Whichever model you choose, download it and place the <code>frozen_inference_graph.pb</code> file in the <code>tensorflow</code> folder in your configuration directory.</p>
<h2>Configuration</h2>
<p>To enable this platform in your installation, add the following to your <code>configuration.yaml</code> file:</p>
<pre><code class="language-yaml"># Example configuration.yaml entry
image_processing:
  - platform: tensorflow
    source:
      - entity_id: camera.local_file
    model:
      graph: /home/homeassistant/.homeassistant/tensorflow/frozen_inference_graph.pb
</code></pre>
<div class="config-vars">
  <h3><a class="title-link" name="configuration-variables" href="#configuration-variables"></a> Configuration Variables</h3>
  <dl class=''><dt><a class='title-link' name='source' href='#source'></a> source</dt><dd><p class='desc'><span class='type'>(<span class='map'>map</span>)</span><span class='required'>(Required)</span><span class='description'><p>The list of image sources.</p>
</span></p></dd><dd><dl class='nested'><dt><a class='title-link' name='entity_id' href='#entity_id'></a> entity_id</dt><dd><p class='desc'><span class='type'>(<span class='string'>string</span>)</span><span class='required'>(Required)</span><span class='description'><p>A camera entity id to get picture from.</p>
</span></p></dd><dt><a class='title-link' name='name' href='#name'></a> name</dt><dd><p class='desc'><span class='type'>(<span class='string'>string</span>)</span><span class='required'>(Optional)</span><span class='description'><p>This parameter allows you to override the name of your <code>image_processing</code> entity.</p>
</span></p></dd></dl></dd><dt><a class='title-link' name='file_out' href='#file_out'></a> file_out</dt><dd><p class='desc'><span class='type'>(<span class='list'>list</span>)</span><span class='required'>(Optional)</span><span class='description'><p>A <a href="/docs/configuration/templating/#processing-incoming-data">template</a> for the integration to save processed images including bounding boxes. <code>camera_entity</code> is available as the <code>entity_id</code> string of the triggered source camera.</p>
</span></p></dd><dt><a class='title-link' name='model' href='#model'></a> model</dt><dd><p class='desc'><span class='type'>(<span class='map'>map</span>)</span><span class='required'>(Required)</span><span class='description'><p>Information about the TensorFlow model.</p>
</span></p></dd><dd><dl class='nested'><dt><a class='title-link' name='graph' href='#graph'></a> graph</dt><dd><p class='desc'><span class='type'>(<span class='string'>string</span>)</span><span class='required'>(Required)</span><span class='description'><p>Full path to <code>frozen_inference_graph.pb</code>.</p>
</span></p></dd><dt><a class='title-link' name='labels' href='#labels'></a> labels</dt><dd><p class='desc'><span class='type'>(<span class='string'>string</span>)</span><span class='required'>(Optional)</span><span class='description'><p>Full path to a <code>*label_map.pbtext</code>.</p>
</span></p><p class='default'>
Default value: <p>tensorflow/object_detection/data/mscoco_label_map.pbtxt</p>
</p></dd><dt><a class='title-link' name='model_dir' href='#model_dir'></a> model_dir</dt><dd><p class='desc'><span class='type'>(<span class='string'>string</span>)</span><span class='required'>(Optional)</span><span class='description'><p>Full path to tensorflow models directory.</p>
</span></p><p class='default'>
Default value: <p>/tensorflow inside config</p>
</p></dd><dt><a class='title-link' name='area' href='#area'></a> area</dt><dd><p class='desc'><span class='type'>(<span class='map'>map</span>)</span><span class='required'>(Optional)</span><span class='description'><p>Custom detection area. Only objects fully in this box will be reported. Top of image is 0, bottom is 1.  Same left to right.</p>
</span></p></dd><dd><dl class='nested'><dt><a class='title-link' name='top' href='#top'></a> top</dt><dd><p class='desc'><span class='type'>(<span class='float'>float</span>)</span><span class='required'>(Optional)</span><span class='description'><p>Top line defined as % from top of image.</p>
</span></p><p class='default'>
Default value: <p>0</p>
</p></dd><dt><a class='title-link' name='left' href='#left'></a> left</dt><dd><p class='desc'><span class='type'>(<span class='float'>float</span>)</span><span class='required'>(Optional)</span><span class='description'><p>Left line defined as % from left of image.</p>
</span></p><p class='default'>
Default value: <p>0</p>
</p></dd><dt><a class='title-link' name='bottom' href='#bottom'></a> bottom</dt><dd><p class='desc'><span class='type'>(<span class='float'>float</span>)</span><span class='required'>(Optional)</span><span class='description'><p>Bottom line defined as % from top of image.</p>
</span></p><p class='default'>
Default value: <p>1</p>
</p></dd><dt><a class='title-link' name='right' href='#right'></a> right</dt><dd><p class='desc'><span class='type'>(<span class='float'>float</span>)</span><span class='required'>(Optional)</span><span class='description'><p>Right line defined as % from left of image.</p>
</span></p><p class='default'>
Default value: <p>1</p>
</p></dd></dl></dd><dt><a class='title-link' name='categories' href='#categories'></a> categories</dt><dd><p class='desc'><span class='type'>(<span class='list'>list</span>)</span><span class='required'>(Optional)</span><span class='description'><p>List of categories to include in object detection. Can be seen in the file provided to <code>labels</code>.</p>
</span></p></dd></dl></dd></dl>
</div>
<p><code>categories</code> can also be defined as dictionary providing an <code>area</code> for each category as seen in the advanced configuration below:</p>
<pre><code class="language-yaml"># Example advanced configuration.yaml entry
image_processing:
  - platform: tensorflow
    source:
      - entity_id: camera.driveway
      - entity_id: camera.backyard
    file_out:
      - &quot;/tmp/{{ camera_entity.split('.')[1] }}_latest.jpg&quot;
      - &quot;/tmp/{{ camera_entity.split('.')[1] }}_{{ now().strftime('%Y%m%d_%H%M%S') }}.jpg&quot;
    model:
      graph: /home/homeassistant/.homeassistant/tensorflow/frozen_inference_graph.pb
      categories:
        - category: person
          area:
            # Exclude top 10% of image
            top: 0.1
            # Exclude right 15% of image
            right: 0.85
        - car
        - truck
</code></pre>
<h2>Optimising resources</h2>
<p><a href="/integrations/image_processing/">Image processing components</a> process the image from a camera at a fixed period given by the <code>scan_interval</code>. This leads to excessive processing if the image on the camera hasnâ€™t changed, as the default <code>scan_interval</code> is 10 seconds. You can override this by adding to your config <code>scan_interval: 10000</code> (setting the interval to 10,000 seconds), and then call the <code>image_processing.scan</code> service when you actually want to perform processing.</p>
<pre><code class="language-yaml"># Example advanced configuration.yaml entry
image_processing:
  - platform: tensorflow
    scan_interval: 10000
    source:
      - entity_id: camera.driveway
      - entity_id: camera.backyard
</code></pre>
<pre><code class="language-yaml"># Example advanced automations.yaml entry
- alias: Tensorflow scanning
  trigger:
     - platform: state
       entity_id:
         - binary_sensor.driveway
  action:
    - service: image_processing.scan
      entity_id: camera.driveway
</code></pre>
:ET